/**
 * =====================================================
 * CAPITAL PLUS AI AGENT - LM STUDIO CLIENT
 * =====================================================
 * Cliente para comunicarse con el servidor local de LM Studio.
 * Env√≠a prompts al modelo LLM y procesa las respuestas.
 */

import type { 
  LMStudioMessage, 
  LMStudioRequest, 
  LMStudioResponse 
} from '@/types/documents';
import { getSystemPrompt, NO_CONTEXT_RESPONSE } from './systemPrompt';
import { fetchWithTimeout, TIMEOUTS } from '@/lib/timeout';

// =====================================================
// CONFIGURACI√ìN
// =====================================================

const LMSTUDIO_BASE_URL = process.env.LMSTUDIO_BASE_URL || 'http://localhost:1234/v1';
const LMSTUDIO_MODEL = process.env.LMSTUDIO_MODEL || 'llama-3.2-3b-instruct';
const DEFAULT_TEMPERATURE = 0.2;
const DEFAULT_MAX_TOKENS = 2048;

// =====================================================
// MAIN FUNCTIONS
// =====================================================

/**
 * Ejecuta una consulta al LLM de LM Studio
 * 
 * @param messages - Array de mensajes (system, user, assistant)
 * @param options - Opciones adicionales (temperature, max_tokens)
 * @returns Respuesta del LLM
 */
export async function runLLM(
  messages: LMStudioMessage[],
  options: {
    temperature?: number;
    max_tokens?: number;
    model?: string;
  } = {}
): Promise<string> {
  const {
    temperature = DEFAULT_TEMPERATURE,
    max_tokens = DEFAULT_MAX_TOKENS,
    model = LMSTUDIO_MODEL,
  } = options;

  const requestBody: LMStudioRequest = {
    model,
    messages,
    temperature,
    max_tokens,
    stream: false,
  };

  try {
    console.log(`ü§ñ Enviando consulta a LM Studio (modelo: ${model})...`);
    
    // Aplicar timeout a la llamada a LM Studio
    const response = await fetchWithTimeout(
      `${LMSTUDIO_BASE_URL}/chat/completions`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      },
      TIMEOUTS.LLM_REQUEST
    );

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`LM Studio error (${response.status}): ${errorText}`);
    }

    const data: LMStudioResponse = await response.json();

    // Debug: Ver la respuesta completa
    console.log('üîç Respuesta de LM Studio:', JSON.stringify(data, null, 2));

    // Extraer el contenido de la respuesta
    const content = data.choices?.[0]?.message?.content;
    
    if (!content) {
      console.error('‚ùå La respuesta no tiene contenido. Estructura recibida:', {
        hasChoices: !!data.choices,
        choicesLength: data.choices?.length,
        firstChoice: data.choices?.[0],
      });
      throw new Error('La respuesta de LM Studio no contiene contenido');
    }

    // Log de tokens utilizados si est√° disponible
    if (data.usage) {
      console.log(`üìä Tokens: prompt=${data.usage.prompt_tokens}, completion=${data.usage.completion_tokens}, total=${data.usage.total_tokens}`);
    }

    console.log('‚úÖ Respuesta de LM Studio recibida');
    return content;
  } catch (error) {
    console.error('‚ùå Error en runLLM:', error);
    throw error;
  }
}

/**
 * Ejecuta una consulta RAG completa
 * Combina el contexto recuperado con la pregunta del usuario
 * 
 * @param query - Pregunta del usuario
 * @param context - Contexto recuperado de Pinecone
 * @param queryType - Tipo de consulta para ajustar el prompt
 * @returns Respuesta del LLM
 */
export async function runRAGQuery(
  query: string,
  context: string,
  queryType?: string,
  memories?: Array<{ topic: string; summary: string; importance: number }>
): Promise<string> {
  // Si no hay contexto, usar respuesta predeterminada
  if (!context || context.trim() === '') {
    console.log('‚ö†Ô∏è No se encontr√≥ contexto relevante');
    return NO_CONTEXT_RESPONSE;
  }

  // Obtener el system prompt apropiado con memorias
  const systemPrompt = await getSystemPrompt(queryType, memories);

  // Construir el mensaje del usuario con el contexto
  const userMessage = `Pregunta: ${query}

Contexto recuperado de la base de conocimientos:
${context}

**INSTRUCCIONES IMPORTANTES SOBRE CITAS:**
- Cada fuente en el contexto est√° numerada como "Fuente 1", "Fuente 2", etc.
- Cuando uses informaci√≥n de una fuente, DEBES incluir una cita num√©rica al final de la oraci√≥n o frase en formato [1], [2], [3], etc.
- El n√∫mero de la cita corresponde al n√∫mero de la fuente (Fuente 1 = [1], Fuente 2 = [2], etc.).
- Si usas informaci√≥n de m√∫ltiples fuentes en la misma oraci√≥n, incluye todas las citas: [1][2].
- Ejemplo: "El precio es de $2,500,000 MXN [1] y est√° disponible en la zona norte [2]."

Por favor, responde la pregunta bas√°ndote en el contexto proporcionado. Si el contexto no contiene suficiente informaci√≥n para responder completamente, ind√≠calo.`;

  const messages: LMStudioMessage[] = [
    { role: 'system', content: systemPrompt },
    { role: 'user', content: userMessage },
  ];

  try {
    const response = await runLLM(messages);
    return response;
  } catch (error) {
    console.error('‚ùå Error en runRAGQuery:', error);
    throw error;
  }
}

/**
 * Verifica si el servidor LM Studio est√° disponible
 * @returns true si el servidor responde, false si no
 */
export async function checkLMStudioHealth(): Promise<boolean> {
  try {
    // Intentar conexi√≥n b√°sica primero
    const response = await fetch(`${LMSTUDIO_BASE_URL}/models`, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json',
      },
    });

    console.log(`üîç LM Studio health check: Status ${response.status}`);

    if (response.ok) {
      const data = await response.json();
      console.log('‚úÖ LM Studio est√° disponible.');
      
      // Verificar si hay modelos disponibles
      if (data.data && Array.isArray(data.data) && data.data.length > 0) {
        console.log(`üìã Modelos cargados (${data.data.length}):`, data.data.map((m: any) => m.id));
      } else {
        console.warn('‚ö†Ô∏è LM Studio est√° corriendo pero NO hay modelos cargados');
      }
      
      return true;
    }

    console.warn(`‚ö†Ô∏è LM Studio respondi√≥ con status ${response.status}`);
    return false;
  } catch (error) {
    console.error('‚ùå LM Studio no est√° disponible:', error);
    console.log(`üí° Verifica que LM Studio est√© corriendo en ${LMSTUDIO_BASE_URL}`);
    return false;
  }
}

/**
 * Obtiene la lista de modelos disponibles en LM Studio
 * @returns Lista de modelos
 */
export async function getAvailableModels(): Promise<string[]> {
  try {
    const response = await fetch(`${LMSTUDIO_BASE_URL}/models`, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json',
      },
    });

    if (!response.ok) {
      throw new Error(`Error obteniendo modelos: ${response.status}`);
    }

    const data = await response.json();
    const models = data.data?.map((m: { id: string }) => m.id) || [];
    
    return models;
  } catch (error) {
    console.error('‚ùå Error obteniendo modelos:', error);
    throw error;
  }
}

/**
 * Ejecuta una consulta simple sin contexto RAG
 * √ötil para saludos, preguntas generales o consultas que no requieren documentos
 * 
 * @param query - Pregunta del usuario
 * @returns Respuesta del LLM
 */
export async function runSimpleQuery(query: string): Promise<string> {
  const systemPrompt = `Eres un asistente virtual de Capital Plus, una empresa inmobiliaria mexicana.

Tu funci√≥n es ayudar a los usuarios con informaci√≥n sobre desarrollos inmobiliarios, pero para consultas simples como saludos, puedes responder de manera amigable y directa.

Para consultas sobre desarrollos espec√≠ficos, precios, amenidades, inventario, etc., el usuario deber√° hacer una pregunta m√°s detallada que requiera buscar en los documentos.

Responde de manera:
- Concisa y profesional
- Amigable y acogedora
- En espa√±ol
- Sin inventar informaci√≥n que no conoces

Si es un saludo, saluda de vuelta y ofrece tu ayuda.`;

  const messages: LMStudioMessage[] = [
    { 
      role: 'system', 
      content: systemPrompt
    },
    { role: 'user', content: query },
  ];

  return runLLM(messages, {
    temperature: 0.7, // Un poco m√°s creativo para saludos
    max_tokens: 150,  // Respuestas m√°s cortas para consultas simples
  });
}

/**
 * Genera un resumen de un texto largo
 * @param text - Texto a resumir
 * @param maxLength - Longitud m√°xima del resumen (en palabras aproximadas)
 * @returns Resumen del texto
 */
export async function summarizeText(
  text: string, 
  maxLength: number = 200
): Promise<string> {
  const messages: LMStudioMessage[] = [
    { 
      role: 'system', 
      content: `Eres un experto en resumir documentos. Genera res√∫menes claros y concisos de aproximadamente ${maxLength} palabras.` 
    },
    { 
      role: 'user', 
      content: `Por favor, resume el siguiente texto:\n\n${text}` 
    },
  ];

  return runLLM(messages, { temperature: 0.3 });
}

// =====================================================
// STREAMING SUPPORT (Para futuras implementaciones)
// =====================================================

/**
 * Ejecuta una consulta con streaming de respuesta
 * Nota: Requiere implementaci√≥n adicional del lado del cliente
 * 
 * @param messages - Array de mensajes
 * @param onChunk - Callback para cada chunk de texto
 */
export async function runLLMStream(
  messages: LMStudioMessage[],
  onChunk: (chunk: string) => void
): Promise<void> {
  const requestBody: LMStudioRequest = {
    model: LMSTUDIO_MODEL,
    messages,
    temperature: DEFAULT_TEMPERATURE,
    stream: true,
  };

  try {
    // Aplicar timeout a la llamada streaming a LM Studio
    const response = await fetchWithTimeout(
      `${LMSTUDIO_BASE_URL}/chat/completions`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      },
      TIMEOUTS.LLM_STREAM
    );

    if (!response.ok) {
      throw new Error(`LM Studio error: ${response.status}`);
    }

    const reader = response.body?.getReader();
    const decoder = new TextDecoder();

    if (!reader) {
      throw new Error('No se pudo obtener el reader del stream');
    }

    while (true) {
      const { done, value } = await reader.read();
      
      if (done) break;

      const chunk = decoder.decode(value, { stream: true });
      const lines = chunk.split('\n').filter(line => line.trim() !== '');

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          
          if (data === '[DONE]') continue;

          try {
            const parsed = JSON.parse(data);
            const content = parsed.choices?.[0]?.delta?.content;
            
            if (content) {
              onChunk(content);
            }
          } catch {
            // Ignorar l√≠neas que no son JSON v√°lido
          }
        }
      }
    }
  } catch (error) {
    console.error('‚ùå Error en runLLMStream:', error);
    throw error;
  }
}

export default {
  runLLM,
  runRAGQuery,
  checkLMStudioHealth,
  getAvailableModels,
  runSimpleQuery,
  summarizeText,
  runLLMStream,
};

